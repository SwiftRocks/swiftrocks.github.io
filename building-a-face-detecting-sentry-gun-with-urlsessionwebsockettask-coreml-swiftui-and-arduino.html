<!DOCTYPE html>
<html lang="en">

  <head>

    <script src="https://use.fontawesome.com/afd448ce82.js"></script>
    
    <!-- Meta Tag -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    
    <!-- SEO -->
    <meta name="author" content="Bruno Rocha">
    <meta name="keywords" content="Software, Engineering, Blog, Posts, iOS, Xcode, Swift, Articles, Tutorials, OBJ-C, Objective-C, Apple">
    <meta name="description" content="Some time ago I created a little side project that involved an Arduino-powered servo motor that menacingly pointed at people's faces with the help of CoreML, mimicking the Team Fortress 2 Engineer's Sentry Gun. With iOS 13, I decided to re-write that using the new Socket APIs and SwiftUI.">
    <meta name="title" content="Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino">
    <meta name="url" content="https://swiftrocks.com/building-a-face-detecting-sentry-gun-with-urlsessionwebsockettask-coreml-swiftui-and-arduino">
    <meta name="image" content="https://swiftrocks.com/images/thumbs/thumb.jpg?4">
    <meta name="copyright" content="Bruno Rocha">
    <meta name="robots" content="index,follow">

    <meta property="og:title" content="Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino"/>
    <meta property="og:image" content="https://swiftrocks.com/images/thumbs/thumb.jpg?4"/>
    <meta property="og:description" content="Some time ago I created a little side project that involved an Arduino-powered servo motor that menacingly pointed at people's faces with the help of CoreML, mimicking the Team Fortress 2 Engineer's Sentry Gun. With iOS 13, I decided to re-write that using the new Socket APIs and SwiftUI."/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://swiftrocks.com/building-a-face-detecting-sentry-gun-with-urlsessionwebsockettask-coreml-swiftui-and-arduino"/>

    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image" content="https://swiftrocks.com/images/thumbs/thumb.jpg?4"/>
    <meta name="twitter:image:alt" content="Page Thumbnail"/>
    <meta name="twitter:title" content="Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino"/>
    <meta name="twitter:description" content="Some time ago I created a little side project that involved an Arduino-powered servo motor that menacingly pointed at people's faces with the help of CoreML, mimicking the Team Fortress 2 Engineer's Sentry Gun. With iOS 13, I decided to re-write that using the new Socket APIs and SwiftUI."/>
    <meta name="twitter:site" content="@rockbruno_"/>
    
    <!-- Favicon -->
    <link rel="icon" type="image/png" href="images/favicon/iconsmall2.png" sizes="32x32" />
    <link rel="apple-touch-icon" href="images/favicon/iconsmall2.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">

    <link rel="canonical" href="https://swiftrocks.com/building-a-face-detecting-sentry-gun-with-urlsessionwebsockettask-coreml-swiftui-and-arduino"/>

  <!-- Bootstrap CSS Plugins --> 
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css">
  <!-- Prism CSS Stylesheet --> 
  <link rel="stylesheet" type="text/css" href="css/prism4.css"> 
  <!-- Main CSS Stylesheet --> 
  <link rel="stylesheet" type="text/css" href="css/style49.css"> 
  <link rel="stylesheet" type="text/css" href="css/sponsor5.css">
    
    <!-- HTML5 shiv and Respond.js support IE8 or Older for HTML5 elements and media queries -->
    <!--[if lt IE 9]>
	   <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
	   <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script type="application/ld+json">
  {
"@context": "https://schema.org",
"@type": "BlogPosting",
"mainEntityOfPage": {
  "@type": "WebPage",
  "@id": "https://swiftrocks.com/building-a-face-detecting-sentry-gun-with-urlsessionwebsockettask-coreml-swiftui-and-arduino"
},
"image": [
  "https://swiftrocks.com/images/thumbs/thumb.jpg"
],
"datePublished": "2019-10-15T18:39:00+00:00",
"dateModified": "2020-04-12T14:00:00+02:00",
"author": {
  "@type": "Person",
  "name": "Bruno Rocha"
},
 "publisher": {
  "@type": "Organization",
  "name": "SwiftRocks",
  "logo": {
    "@type": "ImageObject",
    "url": "https://swiftrocks.com/images/thumbs/thumb.jpg"
  }
},
"headline": "Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino",
    "abstract": "Some time ago I created a little side project that involved an Arduino-powered servo motor that menacingly pointed at people's faces with the help of CoreML, mimicking the Team Fortress 2 Engineer's Sentry Gun. With iOS 13, I decided to re-write that using the new Socket APIs and SwiftUI."
}
  </script>
    

  </head>

 <body>
      
    
    
     
    
    
  <div id="main"> 
<!-- Blog Header --> 
         <!-- Blog Post (Right Sidebar) Start --> 
   <div class="container"> 
                 <div class="col-xs-12">
                    <div class="page-body">
                    	<div class="row">
         <div><a class="logo-link" href="https://swiftrocks.com">
           <img id="logo" class="logo" alt="SwiftRocks" src="images/bg/logo2dark.png">
         </a>
           <div class="menu-large">
           <div class="menu-arrow-right"></div>
           <div class="menu-header menu-header-large">
           <div class="menu-item">
            <a href="blog">blog</a>
           </div>
           <div class="menu-item">
            <a href="about">about</a>
           </div>
           <div class="menu-item">
            <a href="talks">talks</a>
           </div>
           <div class="menu-item">
            <a href="projects">projects</a>
           </div>
           <div class="menu-item">
            <a href="software-engineering-book-recommendations">book recs</a>
           </div>
           <div class="menu-item">
            <a href="games">game recs</a>
           </div>
           <div class="menu-arrow-right-2"></div>
           </div>
           </div>
           <div class="menu-small">
           <div class="menu-arrow-right"></div>
           <div class="menu-header menu-header-small-1">
           <div class="menu-item">
            <a href="blog">blog</a>
           </div>
           <div class="menu-item">
            <a href="about">about</a>
           </div>
           <div class="menu-item">
            <a href="talks">talks</a>
           </div>
           <div class="menu-item">
            <a href="projects">projects</a>
           </div>
           <div class="menu-arrow-right-2"></div>
           </div>
           <div class="menu-arrow-right"></div>
           <div class="menu-header menu-header-small-2">
           <div class="menu-item">
            <a href="software-engineering-book-recommendations">book recs</a>
           </div>
           <div class="menu-item">
            <a href="games">game recs</a>
           </div>
           <div class="menu-arrow-right-2"></div>
           </div>
           </div>
       </div>
                            <div class="content-page" id="WRITEIT_DYNAMIC_CONTENT">
 
  
  <!--WRITEIT_POST_NAME=Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino--> 
  <!--WRITEIT_POST_HTML_NAME=building-a-face-detecting-sentry-gun-with-urlsessionwebsockettask-coreml-swiftui-and-arduino--> 
  <!--WRITEIT_POST_SITEMAP_DATE_LAST_MOD=2020-04-12T14:00:00+02:00--> 
  <!--WRITEIT_POST_SITEMAP_DATE=2019-10-15T18:39:00+00:00--> 
  <!--Add here the additional properties that you want each page to possess.--> 
  <!--These properties can be used to change content in the template page or in the page itself as shown here.--> 
  <!--Properties must start with 'WRITEIT_POST'.--> 
  <!--Writeit provides and injects WRITEIT_POST_NAME and WRITEIT_POST_HTML_NAME by default.--> 
  <!--WRITEIT_POST_SHORT_DESCRIPTION=Some time ago I created a little side project that involved an Arduino-powered servo motor that menacingly pointed at people's faces with the help of CoreML, mimicking the Team Fortress 2 Engineer's Sentry Gun. With iOS 13, I decided to re-write that using the new Socket APIs and SwiftUI.-->
  
  <title>Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino</title>  
 

<div class="blog-post"> 
 <div class="post-title-index">  
  <h1>Building a Face Detecting Robot with URLSessionWebSocketTask, CoreML, SwiftUI and an Arduino</h1>
 </div> 
 <div class="post-info"> 
  <div class="post-info-text">
   Published on 16 Oct 2019 
  </div> 
 </div>   
 <div class="post-image margin-top-40 margin-bottom-40"> 
  <video style="width: 100%; height: auto;" controls> 
   <source src="https://i.imgur.com/g6lhAxR.mp4" type="video/mp4"> Your browser does not support the video tag. 
  </video> 
 </div> 
 <p>iOS 13 marks the release of long-waited features like Dark Mode, but it also brought some needed changes on less popular aspects. Prior to iOS 13, creating socket connections required coding very low-level network interactions which made libraries like Starscream and Socket.io the go-to solution for sockets in iOS. Now with iOS 13, a new native <code>URLSessionWebSocketTask</code> class is available to finally make creating and managing socket connections easier for developers.</p>
 <div class="sponsor-article-ad-auto hidden"></div>
 <p><a href="https://twitter.com/rockbruno_/status/993152346841669632">Some time ago I created a little side project that involved an Arduino-powered servo motor that menacingly pointed at people's faces with the help of CoreML, mimicking the Team Fortress 2 Engineer's Sentry Gun.</a> With iOS 13, I decided to re-write that using the new Socket APIs and SwiftUI.</p>
 <div class="post-image margin-top-40 margin-bottom-40"> 
  <video style="width: 100%; height: auto;" controls> 
   <source src="https://i.imgur.com/g6lhAxR.mp4" type="video/mp4"> Your browser does not support the video tag. 
  </video> 
 </div>
 <p>Although the final project involves an Arduino and a Raspberry Pi, the focus will be the iOS part of it since Swift is the focus of this blog. If at the end you want more info about how the other components are connected, feel free to contact me with questions!</p>
 <h2>Organizing the ideas</h2>
 <p>Since this project has several components, lets detail what needs to be done. To build a face tracking murder robot, we'll need an iOS app that does the following:</p>
 <p> * The app opens the user's camera.</p>
 <p> * Each time the camera's frame is updated, we capture its output.</p>
 <p> * The output is routed to Vision's <code>VNDetectFaceRectanglesRequest</code>.</p>
 <p> * From the results, we draw rectangles on the screen to show where faces were detected.</p>
 <p> * Assuming that the first found face is our target, we calculate the X/Y angles that our robot should face based on the face's bounds on the screen.</p>
 <p> * Using sockets, we'll send these angles to a Raspberry Pi which will be running a server and handling the communication with the Arduino-connected servo motor.</p>
 <p> * While all that happens, we display a HUD with some basic information to the user.</p>
 <h2>Using URLSessionWebSocketTask</h2>
 <p>We can start this project by creating a worker class that handles a socket connection to a generic server. Similar to how regular <code>URLSession</code> tasks are created, we can retrieve an instance of a socket task by calling <code>URLSession.webSocketTask()</code> and passing the URL to the socket server:</p>
 <pre><code>lazy var session = URLSession(configuration: .default,
                              delegate: self,
                              delegateQueue: OperationQueue())

lazy var webSocketTask: URLSessionWebSocketTask = {
    // This is the IP of my Raspberry Pi.
    let url = URL(string: "ws://192.168.15.251:12354")!
    return session.webSocketTask(with: url)
}()</code></pre>
 <p>Although receiving messages from the socket isn't necessary for this project, covering it is important: <code>URLSessionWebSocketTask</code> supports receiving and sending messages in both <code>Data</code> and <code>String</code> formats, and calling <code>receive</code> will allow the client to receive a message from the server:</p>
 <pre><code>func receive() {
    webSocketTask.receive { result in
        switch result {
        case .success(let message):
            switch message {
            case .data(let data):
                print(data)
            case .string(let string):
                print(string)
            }
        case .failure(let error):
            print(error)
        }
    }
}</code></pre>
 <p>An important aspect here is that unlike other Socket solutions for iOS, <code>receive</code> <b>isn't permanent -- once you receive a message, you'll need to call this method again to receive more messages.</b> This is a very weird design decision considering that the point of sockets is to continuously receive and send messages, but it's how it works in iOS 13.</p>
 <p>In a similar fashion, sending messages can be done by calling the <code>send</code> method:</p>
 <pre><code>webSocketTask.send(.string("I love sockets!")) { error in
    if let error = error {
        print(error)
    }
}</code></pre>
 <h2>Connecting to URLSessionWebSocketTask servers</h2>
 <p>Similar to how regular tasks are started, socket tasks can be initiated by calling the <code>resume()</code> method from the task. After the connection is made, the process of receiving and sending messages will start, and updates to the socket's connection will be posted via the <code>URLSessionWebSocketDelegate</code> type. That's a lot easier than pre-iOS 13 socket solutions!</p>
 <p><b>An important server-side aspect of </b><code>URLSessionWebSocketTask</code><b> that is not documented is that Apple expects socket servers to conform to the</b> <a href="https://tools.ietf.org/html/rfc6455">RFC 6455 - Web Socket Protocol</a>. While Starscream and Socket.io allow you to play with sockets in iOS with simple generic Python servers, using <code>URLSessionWebSocketTask</code> will require you to build a server that is capable of handling the protocol's handshakes and data patterns. If the server doesn't conform to RFC 6455, iOS will simply never connect to the server. <a href="https://gist.github.com/rich20bb/4190781">This is an example of a Python server that works with it, which you can use to test the new socket APIs.</a></p>
 <h2>SocketWorker</h2>
 <p>Now that we know how to use iOS 13's new socket class we can build a simple wrapper class to use it. Because we're going to build our UI with SwiftUI, my worker class will inherit from <code>ObservableObject</code> and define a publisher to allow me to route the socket's connection information back to the views. Here's the almost final version of the <code>SocketWorker</code> (sending messages will be added later):</p>
 <pre><code>final class SocketWorker: NSObject, ObservableObject {
    lazy var session = URLSession(configuration: .default,
                                  delegate: self,
                                  delegateQueue: OperationQueue())

    lazy var webSocketTask: URLSessionWebSocketTask = {
        let url = URL(string: "ws://169.254.141.251:12354")!
        return session.webSocketTask(with: url)
    }()

    var lastSentData = ""

    let objectWillChange = ObservableObjectPublisher()

    var isConnected: Bool? = nil {
        willSet {
            DispatchQueue.main.async { [weak self] in
                self?.objectWillChange.send()
            }
        }
    }

    var connectionStatus: String {
        guard let isConnected = isConnected else {
            return "Connecting..."
        }
        if isConnected {
            return "Connected"
        } else {
            return "Disconnected"
        }
    }

    func resume() {
        webSocketTask.resume()
    }

    func suspend() {
        webSocketTask.suspend()
    }
}

extension SocketWorker: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        isConnected = true
    }

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        isConnected = false
    }
}</code></pre>
 <p>It would be nice to display the socket's connection status and camera's targeting information to the user, so we'll build a view to do so. Thankfully SwiftUI allows us to do this quicker than ever:</p>
 <div class="post-image margin-top-40 margin-bottom-40"> 
  <img src="https://i.imgur.com/QrHhYgS.png" alt="" style="max-width: 100%"> 
 </div>
 <pre><code>struct DataView: View {

    let descriptionText: String
    let degreesText: String
    let connectionText: String
    let isConnected: Bool?

    let padding: CGFloat = 32

    var body: some View {
        VStack(alignment: .center) {
            HStack {
                Text(connectionText)
                    .font(.system(.callout))
                    .foregroundColor(isConnected == true ? .green : .red)
                Spacer()
            }
            Spacer()
            VStack(alignment: .center, spacing: 8) {
                Text(descriptionText)
                    .font(.system(.largeTitle))
                Text(degreesText)
                    .font(.system(.title))
            }
        }.padding(padding)
    }
}</code></pre>
 <h2>TargetDataViewModel</h2>
 <p>To allow our <code>DataView</code> to receive the camera's targeting information, we'll create an observable <code>TargetDataViewModel</code> that will be responsible for storing and routing the user's UI strings as well as the actual data that will be sent to the socket.</p>
 <p>Processing the socket data will work by receiving a face's <code>CGRect</code>; based on the face's frame on the screen, we can calculate the X and Y angles that the Arduino's servo motor should be aiming at. In this case since I only have one servo, only the X angle will be sent to the socket. You can send anything to the socket, and for simplicity, I've decided to follow the <code>X{angle}.</code> format. This means that if the servo needs to aim at 90 degrees, the socket will receive a message containing <code>X90.</code>. The dot at the end serves as an "end of command" token, which makes things easier on the Arduino side. If there's no visible face on the screen, we'll send <code>L</code> to the socket (as in, target lost). We'll make this communication work through delegates so we can send this data back to <code>SocketWorker</code>:</p>
 <pre><code>protocol TargetDataViewModelDelegate: AnyObject {
    func targetDataDidChange(_ data: String)
}

class TargetDataViewModel: ObservableObject {

    weak var delegate: TargetDataViewModelDelegate?

    let objectWillChange = ObservableObjectPublisher()

    var targetTitle = "..." {
        willSet {
            self.objectWillChange.send()
        }
    }

    var targetDescription = "..." {
        willSet {
            self.objectWillChange.send()
        }
    }

    func process(target: CGRect?) {
        guard let target = target else {
            targetTitle = "Sentry mode"
            targetDescription = "No targets"
            delegate?.targetDataDidChange("L")
            return
        }
        let oldMin: CGFloat = 0
        let offset: CGFloat = 40
        let newMin: CGFloat = 0 + offset
        let newMax: CGFloat = 180 - offset
        let newRange = newMax - newMin
        func convertToDegrees(position: CGFloat, oldMax: CGFloat) -&gt; Int {
            let oldRange = oldMax - oldMin
            let scaledAngle = (((position - oldMin) * newRange) / oldRange) + newMin
            return Int(scaledAngle)
        }
        let bounds = UIScreen.main.bounds
        let oldMaxX = bounds.width
        let oldMaxY = bounds.height
        let xAngle = convertToDegrees(position: target.midX, oldMax: oldMaxX)
        let yAngle = convertToDegrees(position: target.midY, oldMax: oldMaxY)
        targetTitle = "Shooting"
        targetDescription = "X: \(xAngle) | Y: \(yAngle)"
        let data = "X\(xAngle)."
        delegate?.targetDataDidChange(data)
    }
}</code></pre>
 <p>With the view model in place, we can finish <code>SocketWorker</code> by making it send the target's information to the socket:</p>
 <pre><code>extension SocketWorker: TargetDataViewModelDelegate {
    func targetDataDidChange(_ data: String) {
        // Avoid sending duplicate data to the socket.
        guard data != lastSentData else {
            return
        }
        lastSentData = data
        webSocketTask.send(.string(data)) { error in
            if let error = error {
                print(error)
            }
        }
    }
}</code></pre>
 <h2>Showing the user's camera on the screen</h2>
 <p>Getting access to a device's camera with SwiftUI has a small problem. Although we can easily do it in UIKit using <code>AVCaptureVideoPreviewLayer</code>, the concept of layers doesn't really exist in SwiftUI, so we can't use this layer in normal SwiftUI views. Fortunately, Apple allows you to create bridges between UIKit and SwiftUI so that no functionality is lost.</p>
 <p>In order to display a camera, we'll need to build an old-fashioned UIKit <code>CameraViewController</code> and bridge it to SwiftUI using the <code>UIViewControllerRepresentable</code> protocol. We'll also route our data view model to this view controller so it can update it based on the camera's output:</p>
 <pre><code>struct CameraViewWrapper: UIViewControllerRepresentable {

    typealias UIViewControllerType = CameraViewController
    typealias Context = UIViewControllerRepresentableContext

    let viewController: CameraViewController

    func makeUIViewController(context: Context&lt;CameraViewWrapper&gt;) -&gt; CameraViewController {
        return viewController
    }

    func updateUIViewController(_ uiViewController: CameraViewController, context: Context&lt;CameraViewWrapper&gt;) {}
}

final class CameraViewController: UIViewController, ObservableObject {

    @ObservedObject var targetViewModel: TargetDataViewModel

    init(targetViewModel: TargetDataViewModel) {
        self.targetViewModel = targetViewModel
        super.init(nibName: nil, bundle: nil)
    }

    required init?(coder: NSCoder) {
        fatalError()
    }

    override func loadView() {
        let view = CameraView(delegate: self)
        self.view = view
    }

    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        (view as? CameraView)?.startCaptureSession()
    }

    override func viewWillDisappear(_ animated: Bool) {
        super.viewWillAppear(animated)
        (view as? CameraView)?.stopCaptureSession()
    }
}

extension CameraViewController: CameraViewDelegate {
    func cameraViewDidTarget(frame: CGRect) {
        targetViewModel.process(target: frame, view: view)
    }

    func cameraViewFoundNoTargets() {
        targetViewModel.process(target: nil, view: view)
    }
}</code></pre>
 <p>I will skip the definition of <code>CameraView</code> because the code is massive and there's nothing special about it -- we just add a camera layer to the screen and start recording once the view controller appears. We also define a <code>CameraViewDelegate</code> so that our face detection events can be routed back to our view model. <a href="https://github.com/rockbruno/NSSentryGun">You can see the code for it in the NSSentryGun repo.</a></p>
 <p>Now with the actual face detection being all that's left, we can wrap everything up into our app's main <code>ContentView</code>!</p>
 <pre><code>struct ContentView: View {

    @ObservedObject var socketWorker: SocketWorker
    @ObservedObject var targetViewModel: TargetDataViewModel

    let cameraViewController: CameraViewController

    init() {
        self.socketWorker = SocketWorker()
        let targetViewModel = TargetDataViewModel()
        self.targetViewModel = targetViewModel
        self.cameraViewController = CameraViewController(
            targetViewModel: targetViewModel
        )
        targetViewModel.delegate = socketWorker
    }

    var body: some View {
        ZStack {
            CameraViewWrapper(
                viewController: cameraViewController
            )
            DataView(
                descriptionText: targetViewModel.targetTitle,
                degreesText: targetViewModel.targetDescription,
                connectionText: socketWorker.connectionStatus,
                isConnected: socketWorker.isConnected
            ).expand()
        }.onAppear {
            self.socketWorker.resume()
        }.onDisappear {
            self.socketWorker.suspend()
        }.expand()
    }
}

extension View {
    func expand() -&gt; some View {
        return frame(
            minWidth: 0,
            maxWidth: .infinity,
            minHeight: 0,
            maxHeight: .infinity,
            alignment: .topLeading
        )
    }
}</code></pre>
 <p>After turning on our socket server, running this app will allow us to connect to it and see the user's camera. However, no information will be actually sent since we're doing nothing with the camera's output yet.</p>
 <div class="post-image margin-top-40 margin-bottom-40"> 
  <img src="https://i.imgur.com/lNh5OBu.jpg" alt="" style="max-width: 100%">&gt; 
 </div>
 <h2>Detecting faces in images with CoreML</h2>
 <p>After the release of CoreML, every iOS release adds new capabilities to the built-in <code>Vision</code> framework. In this case, the ability to detect faces in an image can be easily done by performing the <code>VNDetectFaceRectanglesRequest</code> request with the camera's output as a parameter.</p>
 <p>The result of this request is a <code>[VNFaceObservation]</code> array that contains the bounding box of every face detected in the image. We can use this result to draw rectangles around the detected faces and use these rectangles as the input for our previously made <code>TargetDataViewModel</code>. Before defining the request itself, lets first handle the response code inside <code>CameraView</code>:</p>
 <pre><code>func handleFaces(request: VNRequest, error: Error?) {
    DispatchQueue.main.async { [unowned self] in
        guard let results = request.results as? [VNFaceObservation] else {
            return
        }
        // We'll add all drawn rectangles in a `maskLayer` property
        // and remove them when we get new responses.
        for mask in self.maskLayer {
            mask.removeFromSuperlayer()
        }
        let frames: [CGRect] = results.map {
            let transform = CGAffineTransform(scaleX: 1, y: -1)
                                .translatedBy(x: 0, y: -self.frame.height)
            let translate = CGAffineTransform
                                .identity
                                .scaledBy(x: self.frame.width, y: self.frame.height)
            return $0.boundingBox
                        .applying(translate)
                        .applying(transform)
        }
        frames
            .sorted { ($0.width * $0.height) &gt; ($1.width * $1.height) }
            .enumerated()
            .forEach(self.drawFaceBox)
        guard let targetRect = results.first else {
            self.delegate.cameraViewFoundNoTargets()
            return
        }
        delegate.cameraViewDidTarget(frame: targetRect)
    }
}</code></pre>
 <p>(The <code>boundingBox</code> property contains values from zero to one with the y-axis going up from the bottom, so this method also needs to handle scaling and reversing the y-axis for the box to match the camera's bounds.)</p>
 <p>After processing the face's rectangle, we sort them by total area and draw them on the screen. We'll consider the rectangle with the biggest area as our target.</p>
 <pre><code>func drawFaceBox(index: Int, frame: CGRect) {
    let color = index == 0 ? UIColor.red.cgColor : UIColor.yellow.cgColor
    createLayer(in: frame, color: UIColor.red.cgColor)
}

private func createLayer(in rect: CGRect, color: CGColor) {
    let mask = CAShapeLayer()
    mask.frame = rect
    mask.opacity = 1
    mask.borderColor = color
    mask.borderWidth = 2
    maskLayer.append(mask)
    layer.insertSublayer(mask, at: 1)
}</code></pre>
 <p>To perform the request, we need access to the camera's image buffer. This can be done by setting our <code>CameraView</code> as the layer's <code>AVCaptureVideoDataOutput</code> delegate, which has a method containing the camera's buffer. We can then convert the buffer into Vision's expected <code>CVImageBuffer</code> input and finally perform the request:</p>
 <pre><code>extension CameraView: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer),
              let exifOrientation = CGImagePropertyOrientation(rawValue: 0) else
        {
            return
        }
        var requestOptions: [VNImageOption : Any] = [:]
        let key = kCMSampleBufferAttachmentKey_CameraIntrinsicMatrix
        if let cameraIntrinsicData = CMGetAttachment(sampleBuffer, key: key, attachmentModeOut: nil) {
            requestOptions = [.cameraIntrinsics: cameraIntrinsicData]
        }
        let imageRequestHandler = VNImageRequestHandler(
            cvPixelBuffer: pixelBuffer,
            orientation: exifOrientation,
            options: requestOptions
       )
        do {
            let request = VNDetectFaceRectanglesRequest(completionHandler: handleFaces)
            try imageRequestHandler.perform([request])
        } catch {
            print(error)
        }
    }
}</code></pre>
 <p>If we run the app now and point at something, we'll see rectangles being drawn around faces!</p>
 <div class="post-image margin-top-40 margin-bottom-40"> 
  <img src="https://i.imgur.com/kQtDmQ5.jpg" alt="" style="max-width: 100%"> 
 </div>
 <p>And because we're using <code>ObservableObjects</code>, the processing that happens inside <code>TargetDataViewModel</code> will be published back to the <code>SocketWorker</code>, which will finally send our target's information to the server.</p>
 <div class="post-image"> 
  <img src="https://i.imgur.com/AJZyMyE.png" alt="" style="max-width: 100%"> 
 </div>
 <h2>Beyond iOS: Making an Arduino receive this information</h2>
 <p>As stated at the beginning of the article I won't go too deep on the components that aren't iOS related, but if you're wondering how sending the information to a socket results in a servo motor being moved, we'll use this section to talk more about it.</p>
 <p>Arduinos can communicate with other devices through its serial port, which is connected as a USB device on the other end. With the Arduino connected to your server (I used a Raspberry Pi for the video in the introduction, but simply running the server script on your Mac works as well), you can use Python's <code>serial</code> library to open a connection with the Arduino:</p>
 <pre><code>import serial
ser = serial.Serial('/dev/cu.usbmodem1412401', 9600)</code></pre>
 <p>In this case, <code>/dev/cu.usbmodem1412401</code> is the name of the port where my Arduino is connected, and 9600 is the baud rate expected by the Arduino.</p>
 <p>When the server receives data from the iOS app, we can write it into the serial port:</p>
 <pre><code>while True:
    newData = client.recv(4096)
    msg = self.connections[fileno].recover(newData)
    if not msg: continue
    print msg
    ser.write(msg)</code></pre>
 <p>If the Arduino is listening to its serial port, it will be able to receive this data and parse it into something meaningful. With a servo motor connected to pin 9, here's the Arduino C code to parse the little rules we created when processing the data in the view model:</p>
 <pre><code>#include &lt;Servo.h&gt;

Servo myservo;

bool isScanning = false;

void setup() {
  Serial.begin(9600);
  myservo.attach(9);
  myservo.write(90);
  delay(1000);
}

void loop() {
  if (isScanning == true) {
    scan();
    return;
  }
  if (Serial.available() &gt; 0) {
    char incomingByte = Serial.read();
    if (incomingByte == 'L') {
      isScanning = true;
    } else if (incomingByte == 'X') { // else if
      setServoAngle();
    } else { //Unwanted data, get rid of it
      while(Serial.read() != -1) {
        incomingByte = Serial.read();
      }
    }
  }
}

void scan() {
  int angle = myservo.read();
  for (int i = angle; i&lt;= 160; i++) {
    if (Serial.peek() != -1) {
      isScanning = false;
      return;
    }
    myservo.write(i);
    delay(15);
  }
  for (int i = 160; i&gt;= 20; i--) {
    if (Serial.peek() != -1) {
      isScanning = false;
      return;
    }
    myservo.write(i);
    delay(15);
  }
}

void setServoAngle() {
  unsigned int integerValue = 0;
  while(1) {
    char incomingByte = Serial.read();
    if (incomingByte == -1) {
      continue;
    }
    if (isdigit(incomingByte) == false) {
      break;
    }
    integerValue *= 10;
    integerValue = ((incomingByte - 48) + integerValue);
  }
  if (integerValue &gt;= 0 &amp;&amp; integerValue &lt;= 180) {
      myservo.write(integerValue);
  }
}</code></pre>
 <p>To make it short, Arduino will enter a "scan mode" that makes the servo wiggle back and forth if an <code>L</code> is received from the server. This goes on until an <code>X</code> is received -- when this happens, the Arduino then parses the numeric component of the command and sets it as the servo's angle. All that's left now is to attach a Nerf gun to it and make it actually shoot intruders!</p>
 <h2>Conclusion</h2>
 <div class="sponsor-article-ad-auto hidden"></div>
 <p>The introduction of the new URLSessionWebSocketTask finally solves a major pain point in developing networking applications for iOS, and I hope this article sparks some ideas for new side projects for you. Creating side projects is a cool way to learn new technologies -- I first did this project to know more about Raspberry Pis, and this article was a chance for me to have my first interaction with SwiftUI and Combine after Xcode 11's release.</p>
 <p>Follow me on my Twitter (<a href="https://twitter.com/rockbruno_">@rockbruno_</a>), and let me know of any suggestions and corrections you want to share.</p>
 <h2>References and Good reads</h2>
 <a href="https://developer.apple.com/documentation/foundation/urlsessionwebsockettask">URLSessionWebSocketTask</a>
 <br>
 <a href="https://tools.ietf.org/html/rfc6455">RFC 6455 - The WebSocket Protocol</a>
 <br>
 <a href="https://gist.github.com/rich20bb/4190781">Python implementation of an RFC 6455 socket server</a>
 <br>
</div></div>
                              
   

    <div class="blog-post footer-main">
      <div class="footer-logos">
        <a href="https://swiftrocks.com/rss.xml"><i class="fa fa-rss"></i></a>
        <a href="https://twitter.com/rockbruno_"><i class="fa fa-twitter"></i></a>
        <a href="https://github.com/rockbruno"><i class="fa fa-github"></i></a>
      </div>
      <div class="footer-text">
        © 2025 Bruno Rocha
      </div>
      <div class="footer-text">
        <p><a href="https://swiftrocks.com">Home</a>       /        <a href="blog">See all posts</a></p>
      </div>
      </div>
    </div>

                         </div>

</div>
                        
                           
                         </div>
                     
                     
                  </div>
                  <!-- Blog Post (Right Sidebar) End -->
                
            </div>
         </div>
      </div>
    
    
    <!-- All Javascript Plugins  -->
  <script type="text/javascript" src="js/jquery.min.js"></script> 
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
  <script type="text/javascript" src="js/prism5.js"></script> 
    <!-- Main Javascript File  -->
    <script type="text/javascript" src="js/scripts30.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-H8KZTWSQ1R"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-H8KZTWSQ1R');
</script>

   </body>
 </html>
